{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2718be7-64ec-46a0-8867-1e0c8fa01a03",
   "metadata": {},
   "source": [
    "**CI/CD Pipeline for ML Projects**\n",
    "\n",
    "In this notebook, we will build a **CI/CD pipeline for ML projects**.  \n",
    "The goal is to automate the process from **data preparation → model training → evaluation → deployment**.\n",
    "\n",
    "**1. Why CI/CD for ML?**\n",
    "\n",
    "- **CI (Continuous Integration):** Every time code changes, it is automatically tested, validated, and merged.  \n",
    "- **CD (Continuous Delivery/Deployment):** New versions of the model/code are automatically delivered or deployed to production.  \n",
    "\n",
    "**Tools we use:**\n",
    "- **Git + GitHub** → Version control & code hosting.  \n",
    "- **GitHub Actions (CI/CD)** → Automates testing, training, and deployment.  \n",
    "- **Docker / Cloud (AWS, Azure, GCP)** → For real deployments (optional in this demo).  \n",
    "\n",
    "**2. Workflow Steps**\n",
    "\n",
    "1. Data extraction & cleaning.  \n",
    "2. Train/Test split (`X_train`, `y_train`, `X_test`, `y_test`).  \n",
    "3. Build models (Logistic Regression, Random Forest).  \n",
    "4. Evaluate (confusion matrix, f1, precision, recall, accuracy).  \n",
    "5. Save outputs (plots + metrics + pickle file).  \n",
    "6. Push code & files to GitHub.  \n",
    "7. Create a GitHub Actions Workflow (`.github/workflows/run.yml`).  \n",
    "8. CI/CD auto-runs pipeline on every push.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897ffb1-bbff-4a31-bac1-1b4bbad9269f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208135a9-bd9e-4bd5-ae62-5e984c3a5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score, recall_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86658f19-ac29-4d88-9469-597fab91265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white')\n",
    "\n",
    "# Load Data\n",
    "dataset = pd.read_csv(r\"C:\\Users\\Lenovo\\OneDrive\\Desktop\\Python Everyday work\\Class work\\AI\\MlOps\\CICD\\code-2\\code-2\\iris.csv\")\n",
    "\n",
    "# Feature names (Ensure no extra spaces or parentheses)\n",
    "dataset.columns = [colname.strip(' (cm)').replace(\" \", \"_\") for colname in dataset.columns.tolist()]\n",
    "features_names = dataset.columns.tolist()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3e346c-34e7-488c-a929-41b93c03ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "dataset['sepal_length_width_ratio'] = dataset['sepal_length'] / dataset['sepal_width']\n",
    "dataset['petal_length_width_ratio'] = dataset['petal_length'] / dataset['petal_width']\n",
    "\n",
    "# Select Features\n",
    "dataset = dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', \n",
    "                   'sepal_length_width_ratio', 'petal_length_width_ratio', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1805e9b4-e341-4cb2-b35a-4095abcf9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=44)\n",
    "X_train = train_data.drop('target', axis=1).values.astype('float32')\n",
    "y_train = train_data.loc[:, 'target'].values.astype('int32')\n",
    "X_test = test_data.drop('target', axis=1).values.astype('float32')\n",
    "y_test = test_data.loc[:, 'target'].values.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b30355-7e29-46e2-8b9c-f797389ea919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1264: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression(C=0.0001, solver='lbfgs', max_iter=100, multi_class='multinomial')\n",
    "logreg.fit(X_train, y_train)\n",
    "predictions_lr = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7581ae71-2eef-4069-bbfa-b4cc858441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_lr = confusion_matrix(y_test, predictions_lr)\n",
    "f1_lr = f1_score(y_test, predictions_lr, average='micro')\n",
    "prec_lr = precision_score(y_test, predictions_lr, average='micro')\n",
    "recall_lr = recall_score(y_test, predictions_lr, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18fe76fd-fcf9-4dc1-8f49-2b1bf99aa3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_lr = logreg.score(X_train, y_train) * 100\n",
    "test_acc_lr = logreg.score(X_test, y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a341602d-329b-45b8-aa9c-04e17137a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "predictions_rf = rf_reg.predict(X_test)\n",
    "predictions_rf_class = np.round(predictions_rf).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7180cb7f-6d44-477c-930d-d4176b74692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rf = f1_score(y_test, predictions_rf_class, average='micro')\n",
    "prec_rf = precision_score(y_test, predictions_rf_class, average='micro')\n",
    "recall_rf = recall_score(y_test, predictions_rf_class, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30965bff-d967-4e13-ae69-bf0c08bbeac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_rf = rf_reg.score(X_train, y_train) * 100\n",
    "test_acc_rf = rf_reg.score(X_test, y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a539f589-bd34-4da2-b20b-78084012ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_reg, f)\n",
    "\n",
    "# Save Scores\n",
    "with open(\"scores.txt\", \"w\") as score:\n",
    "    score.write(f\"Random Forest Train Accuracy: {train_acc_rf:.2f}%\\n\")\n",
    "    score.write(f\"Random Forest Test Accuracy: {test_acc_rf:.2f}%\\n\")\n",
    "    score.write(f\"F1 Score: {f1_rf:.2f}\\n\")\n",
    "    score.write(f\"Recall Score: {recall_rf:.2f}\\n\")\n",
    "    score.write(f\"Precision Score: {prec_rf:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefb461-2b76-42d6-afae-962a746b7f63",
   "metadata": {},
   "source": [
    "# GitHub Setup for CI/CD\n",
    "\n",
    "1. Create new repo → `cicdpipeline`  \n",
    "2. Upload files:\n",
    "   - `iris.csv`\n",
    "   - `train.py` (the model training code above)\n",
    "   - `requirements.txt`\n",
    "\n",
    "`requirements.txt` example:\n",
    "```\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "matplotlib\n",
    "seaborn\n",
    "```\n",
    "\n",
    "3. Inside repo, create folder: `.github/workflows/`  \n",
    "4. Add file `run.yml`:\n",
    "```yaml\n",
    "- name: Set up Python\n",
    "  uses: actions/setup-python@v4\n",
    "  with:\n",
    "    python-version: '3.10'\n",
    "\n",
    "- name: Install dependencies\n",
    "  run: |\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "- name: Run training script\n",
    "  run: |\n",
    "    python train.py\n",
    "```\n",
    "\n",
    "5. Commit & Push → Go to **Actions** tab → See pipeline running.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f655e8-0691-484b-8e1e-bbf81cd4dcb7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We successfully:\n",
    "- Trained ML models (Logistic Regression & Random Forest).  \n",
    "- Saved metrics and model artifacts.  \n",
    "- Set up GitHub Actions CI/CD workflow.  \n",
    "- Automated model training on each push.  \n",
    "\n",
    "This demonstrates a **basic MLOps CI/CD pipeline** which can be extended with:  \n",
    "- Docker + Kubernetes for deployment.  \n",
    "- Monitoring with Prometheus / Grafana.  \n",
    "- Full ML lifecycle automation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca3ad6-f12d-4f1c-ac12-45af4cc8b4a0",
   "metadata": {},
   "source": [
    "# Repository Setup\n",
    "**Step 1: Repository Setup** \n",
    "\n",
    "We created a new GitHub repository **CICD_Pipeline** and uploaded the required files.  \n",
    "\n",
    "![Repository Setup](Output1.PNG)  \n",
    "\n",
    "Here we can see:  \n",
    "- `iris.csv` → Dataset  \n",
    "- `train_model.py` → Model training script  \n",
    "- `requirements.txt` → Dependencies  \n",
    "- `.github/workflows/run.yaml` → Workflow definition  \n",
    "\n",
    "# Repository Setup\n",
    "**Step 2: GitHub Actions Workflow** \n",
    "\n",
    "GitHub Actions automatically triggered the workflow whenever changes were pushed.  \n",
    "\n",
    "![Workflow Runs](Output2.PNG)  \n",
    "\n",
    "- The first workflow (`Create run.yaml`)  failed because of a setup issue.  \n",
    "- After fixing, the second workflow (`Update train_model.py`)  passed successfully.  \n",
    "\n",
    "# Pipeline Capturing Code Changes\n",
    "**Step 3: Pipeline Capturing Code Changes**  \n",
    "\n",
    "The pipeline also captures even **small code changes**.  \n",
    "\n",
    "![Pipeline Output](Output3.PNG)  \n",
    "\n",
    "Here we see that:  \n",
    "- **1 file was changed**  \n",
    "- **+2 lines added, -1 line removed**  \n",
    "- GitHub Actions re-ran the pipeline automatically.  \n",
    "\n",
    "This shows the **Continuous Integration (CI)** in action.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
